{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!gdown 1L6a_lc4yzCYU_FTWYyy32anZoZtY3Q66"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FztqpkgMZhrc",
        "outputId": "51055011-ade2-4386-8b71-b7b15af28615"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1L6a_lc4yzCYU_FTWYyy32anZoZtY3Q66\n",
            "To: /content/amazon_top500.csv\n",
            "\r  0% 0.00/288k [00:00<?, ?B/s]\r100% 288k/288k [00:00<00:00, 8.52MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LRoAi69LMso",
        "outputId": "2ef7c5a1-28c4-4762-83a3-9fd1f0d8d85b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAOVXO2kZlGi",
        "outputId": "448598c8-539c-47c1-bd1c-44c9cd364ea6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0BiwWHyDLU-6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data/amazon -p\n",
        "!cp amazon_top500.csv data/amazon"
      ],
      "metadata": {
        "id": "toJI8Mw0Zn4d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 32000\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True"
      ],
      "metadata": {
        "id": "dGLNsgl2aFHz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "import logging\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False\"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n",
        "        if not self.flash:\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\n",
        "                \"bias\",\n",
        "                torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
        "                    1, 1, config.block_size, config.block_size\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = (\n",
        "            x.size()\n",
        "        )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
        "            1, 2\n",
        "        )  # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
        "            1, 2\n",
        "        )  # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
        "            1, 2\n",
        "        )  # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(\n",
        "                q,\n",
        "                k,\n",
        "                v,\n",
        "                attn_mask=None,\n",
        "                dropout_p=self.dropout if self.training else 0,\n",
        "                is_causal=True,\n",
        "            )\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = (\n",
        "            y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        )  # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            dict(\n",
        "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
        "                drop=nn.Dropout(config.dropout),\n",
        "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "                ln_f=LayerNorm(config.n_embd, bias=config.bias),\n",
        "            )\n",
        "        )\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith(\"c_proj.weight\"):\n",
        "                torch.nn.init.normal_(\n",
        "                    p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer)\n",
        "                )\n",
        "\n",
        "        # report number of parameters\n",
        "        logger.info(\"number of parameters: %.2fM\" % (self.get_num_params() / 1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)  # shape (t)\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1\n",
        "            )\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(\n",
        "                x[:, [-1], :]\n",
        "            )  # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        # decrease the block size if necessary\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(\n",
        "            self.transformer.wpe.weight[:block_size]\n",
        "        )\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, \"bias\"):\n",
        "                block.attn.bias = block.attn.bias[:, :, :block_size, :block_size]\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
        "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        logger.info(\n",
        "            f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\"\n",
        "        )\n",
        "        logger.info(\n",
        "            f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\"\n",
        "        )\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            optim_groups, lr=learning_rate, betas=betas, **extra_args\n",
        "        )\n",
        "        logger.info(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\"estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS\"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd // cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6 * N + 12 * L * H * Q * T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0 / dt)  # per second\n",
        "        flops_promised = 312e12  # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = (\n",
        "                idx\n",
        "                if idx.size(1) <= self.config.block_size\n",
        "                else idx[:, -self.config.block_size :]\n",
        "            )\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "V9wEQmtDaJsP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2IIa309FZWRT"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "DATASET_PATH = \"/content/data/amazon/amazon_top500.csv\"\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "\n",
        "df_str = \"\\n\".join(df[df.columns].apply(lambda x: \",\".join(x.astype(str)), axis=1))\n",
        "\n",
        "with open(\"/content/data/amazon/amazon_top500.txt\", \"w\") as f:\n",
        "    f.write(df_str)\n",
        "\n",
        "\n",
        "with open(DATASET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "n = len(data)\n",
        "logger.info(f\"Data has {n:,} characters\")\n",
        "train_data = data[: int(n * 0.9)]\n",
        "val_data = data[int(n * 0.9) :]\n",
        "\n",
        "# encode with tiktoken gpt2 bpe\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "train_ids = enc.encode_ordinary(train_data)\n",
        "val_ids = enc.encode_ordinary(val_data)\n",
        "\n",
        "logger.info(f\"train has {len(train_ids):,} tokens\")\n",
        "logger.info(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "logger.info(\"Exporting to bin files\")\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "\n",
        "train_ids.tofile(\"/content/data/amazon/train.bin\")\n",
        "val_ids.tofile(\"/content/data/amazon/val.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.distributed import destroy_process_group, init_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "# I/O\n",
        "google_drive_mounted = True\n",
        "out_dir = \"out\" if not google_drive_mounted else \"drive/MyDrive/model_out\"\n",
        "eval_interval = 32\n",
        "log_interval = 1\n",
        "eval_iters = 3\n",
        "eval_only = False  # if True, script exits right after the first eval\n",
        "always_save_checkpoint = True  # if True, always save a checkpoint after each eval\n",
        "init_from = \"scratch\"  # 'scratch' or 'resume'\n",
        "\n",
        "# wandb logging\n",
        "wandb_log = False  # disabled by default\n",
        "wandb_project = \"owt\"\n",
        "wandb_run_name = \"gpt2\"  # 'run' + str(time.time())\n",
        "\n",
        "# data\n",
        "dataset = \"amazon\"\n",
        "gradient_accumulation_steps = 5 * 8  # used to simulate larger batch sizes\n",
        "batch_size = 6  # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 1024\n",
        "\n",
        "# model\n",
        "n_layer = 12\n",
        "n_head = 12\n",
        "n_embd = 768\n",
        "dropout = 0.0  # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
        "\n",
        "# adamw optimizer\n",
        "learning_rate = 6e-4  # max learning rate\n",
        "max_iters = 500  # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0  # clip gradients at this value, or disable if == 0.0\n",
        "\n",
        "# learning rate decay settings\n",
        "decay_lr = True  # whether to decay the learning rate\n",
        "warmup_iters = 5  # how many steps to warm up for\n",
        "lr_decay_iters = 500  # should be ~= max_iters per Chinchilla\n",
        "min_lr = 6e-5  # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "\n",
        "# DDP settings\n",
        "backend = \"nccl\"  # 'nccl', 'gloo', etc.\n",
        "\n",
        "# system\n",
        "device = \"cuda\"\n",
        "\n",
        "dtype = (\n",
        "    \"bfloat16\"\n",
        "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "    else \"float16\"\n",
        ")  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True  # use PyTorch 2.0 to compile the model to be faster\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [\n",
        "    k\n",
        "    for k, v in globals().items()\n",
        "    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n",
        "]\n",
        "config = {k: globals()[k] for k in config_keys}\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "ddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?\n",
        "\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ[\"RANK\"])\n",
        "    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
        "    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "    device = f\"cuda:{ddp_local_rank}\"\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0  # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank  # each process gets a different seed\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
        "device_type = \"cuda\" if \"cuda\" in device else \"cpu\"\n",
        "\n",
        "ptdtype = {\n",
        "    \"float32\": torch.float32,\n",
        "    \"bfloat16\": torch.bfloat16,\n",
        "    \"float16\": torch.float16,\n",
        "}[dtype]\n",
        "\n",
        "ctx = (\n",
        "    nullcontext()\n",
        "    if device_type == \"cpu\"\n",
        "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        ")\n",
        "\n",
        "\n",
        "data_dir = os.path.join(\"data\", dataset)\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    if split == \"train\":\n",
        "        data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack(\n",
        "        [torch.from_numpy((data[i : i + block_size]).astype(np.int64)) for i in ix]\n",
        "    )\n",
        "    y = torch.stack(\n",
        "        [\n",
        "            torch.from_numpy((data[i + 1 : i + 1 + block_size]).astype(np.int64))\n",
        "            for i in ix\n",
        "        ]\n",
        "    )\n",
        "    if device_type == \"cuda\":\n",
        "        x, y = (\n",
        "            x.pin_memory().to(device, non_blocking=True),\n",
        "            y.pin_memory().to(device, non_blocking=True),\n",
        "        )\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(data_dir, \"meta.pkl\")\n",
        "meta_vocab_size = None\n",
        "\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, \"rb\") as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta[\"vocab_size\"]\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "model_args = dict(\n",
        "    n_layer=n_layer,\n",
        "    n_head=n_head,\n",
        "    n_embd=n_embd,\n",
        "    block_size=block_size,\n",
        "    bias=bias,\n",
        "    vocab_size=None,\n",
        "    dropout=dropout,\n",
        ")\n",
        "\n",
        "if init_from == \"scratch\":\n",
        "    # init a new model from scratch\n",
        "    model_args[\"vocab_size\"] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "\n",
        "    model_conf = ModelConfig(**model_args)\n",
        "    model = Model(model_conf)\n",
        "\n",
        "elif init_from == \"resume\":\n",
        "    print(f\"Resuming training from {out_dir}\")\n",
        "\n",
        "    ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint[\"model_args\"]\n",
        "    # force these config attributes to be equal otherwise we can't even resume training\n",
        "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
        "    for k in [\"n_layer\", \"n_head\", \"n_embd\", \"block_size\", \"bias\", \"vocab_size\"]:\n",
        "        model_args[k] = checkpoint_model_args[k]\n",
        "\n",
        "    model_conf = ModelConfig(**model_args)\n",
        "    model = Model(model_conf)\n",
        "    state_dict = checkpoint[\"model\"]\n",
        "    unwanted_prefix = \"_orig_mod.\"\n",
        "    for k, v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "    iter_num = checkpoint[\"iter_num\"]\n",
        "    best_val_loss = checkpoint[\"best_val_loss\"]\n",
        "\n",
        "# crop down the model block size if desired, using model surgery\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args[\"block_size\"] = (\n",
        "        block_size\n",
        "    )\n",
        "model.to(device)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay, learning_rate, (beta1, beta2), device_type\n",
        ")\n",
        "if init_from == \"resume\":\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "checkpoint = None  # free up memory\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    un_optimized_model = model\n",
        "    model = torch.compile(model)\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "\n",
        "# training loop\n",
        "X, Y = get_batch(\"train\")  # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0\n",
        "raw_model = model.module if ddp else model\n",
        "running_mfu = -1.0\n",
        "\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(\n",
        "            f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
        "        )\n",
        "        if wandb_log:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"iter\": iter_num,\n",
        "                    \"train/loss\": losses[\"train\"],\n",
        "                    \"val/loss\": losses[\"val\"],\n",
        "                    \"lr\": lr,\n",
        "                    \"mfu\": running_mfu * 100,  # convert to percentage\n",
        "                }\n",
        "            )\n",
        "        if losses[\"val\"] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses[\"val\"]\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    \"model\": raw_model.state_dict(),\n",
        "                    \"optimizer\": optimizer.state_dict(),\n",
        "                    \"model_args\": model_args,\n",
        "                    \"iter_num\": iter_num,\n",
        "                    \"best_val_loss\": best_val_loss,\n",
        "                    \"config\": config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, \"ckpt.pt\"))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            model.require_backward_grad_sync = (\n",
        "                micro_step == gradient_accumulation_steps - 1\n",
        "            )\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = (\n",
        "                loss / gradient_accumulation_steps\n",
        "            )  # scale the loss to account for gradient accumulation\n",
        "        X, Y = get_batch(\"train\")\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5:  # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
        "        print(\n",
        "            f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\"\n",
        "        )\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtFSrmvSaVsV",
        "outputId": "6b55dd22-332c-4643-8537-d4d89756dfaf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 245,760\n",
            "compiling the model... (takes a ~minute)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 10.9344, val loss 11.0332\n",
            "iter 0: loss 10.9355, time 90711.23ms, mfu -100.00%\n",
            "iter 1: loss 10.9392, time 12447.62ms, mfu -100.00%\n",
            "iter 2: loss 10.2515, time 13638.47ms, mfu -100.00%\n",
            "iter 3: loss 9.7620, time 13869.93ms, mfu -100.00%\n",
            "iter 4: loss 9.3661, time 13834.00ms, mfu -100.00%\n",
            "iter 5: loss 9.9357, time 13701.38ms, mfu 4.91%\n",
            "iter 6: loss 9.0487, time 13206.45ms, mfu 4.93%\n",
            "iter 7: loss 8.3522, time 13142.06ms, mfu 4.95%\n",
            "iter 8: loss 8.0921, time 13095.82ms, mfu 4.97%\n",
            "iter 9: loss 7.7839, time 13116.00ms, mfu 4.99%\n",
            "iter 10: loss 7.2956, time 13144.01ms, mfu 5.00%\n",
            "iter 11: loss 7.2426, time 13123.50ms, mfu 5.01%\n",
            "iter 12: loss 7.2074, time 13151.28ms, mfu 5.02%\n",
            "iter 13: loss 7.1999, time 13157.81ms, mfu 5.03%\n",
            "iter 14: loss 6.6263, time 13159.53ms, mfu 5.04%\n",
            "iter 15: loss 7.1834, time 13143.05ms, mfu 5.05%\n",
            "iter 16: loss 6.9410, time 13048.83ms, mfu 5.06%\n",
            "iter 17: loss 6.6364, time 12977.46ms, mfu 5.07%\n",
            "iter 18: loss 6.7746, time 12947.95ms, mfu 5.09%\n",
            "iter 19: loss 7.0006, time 12956.55ms, mfu 5.10%\n",
            "iter 20: loss 7.0023, time 12960.34ms, mfu 5.11%\n",
            "iter 21: loss 6.6713, time 12953.53ms, mfu 5.12%\n",
            "iter 22: loss 6.3872, time 12969.92ms, mfu 5.12%\n",
            "iter 23: loss 6.7519, time 12922.28ms, mfu 5.13%\n",
            "iter 24: loss 6.8560, time 12918.11ms, mfu 5.14%\n",
            "iter 25: loss 6.9393, time 12866.90ms, mfu 5.15%\n",
            "iter 26: loss 6.4126, time 12858.48ms, mfu 5.16%\n",
            "iter 27: loss 6.4977, time 12824.47ms, mfu 5.17%\n",
            "iter 28: loss 6.9188, time 12883.30ms, mfu 5.17%\n",
            "iter 29: loss 6.8358, time 12918.33ms, mfu 5.18%\n",
            "iter 30: loss 6.7238, time 12872.44ms, mfu 5.18%\n",
            "iter 31: loss 6.9886, time 12848.87ms, mfu 5.19%\n",
            "step 32: train loss 6.8753, val loss 7.6478\n",
            "saving checkpoint to drive/MyDrive/model_out\n",
            "iter 32: loss 6.9517, time 22914.99ms, mfu 4.96%\n",
            "iter 33: loss 6.8679, time 13151.43ms, mfu 4.98%\n",
            "iter 34: loss 6.6546, time 13258.51ms, mfu 4.99%\n",
            "iter 35: loss 6.8825, time 13056.92ms, mfu 5.01%\n",
            "iter 36: loss 6.9870, time 12886.70ms, mfu 5.03%\n",
            "iter 37: loss 6.8364, time 12805.98ms, mfu 5.05%\n",
            "iter 38: loss 6.5958, time 12853.39ms, mfu 5.07%\n",
            "iter 39: loss 6.6867, time 13021.95ms, mfu 5.08%\n",
            "iter 40: loss 6.8206, time 13002.40ms, mfu 5.09%\n",
            "iter 41: loss 6.6990, time 13072.16ms, mfu 5.10%\n",
            "iter 42: loss 6.9075, time 13021.94ms, mfu 5.10%\n",
            "iter 43: loss 6.7480, time 12984.82ms, mfu 5.11%\n",
            "iter 44: loss 5.7839, time 12952.18ms, mfu 5.12%\n",
            "iter 45: loss 6.8345, time 12893.43ms, mfu 5.13%\n",
            "iter 46: loss 6.4917, time 12894.50ms, mfu 5.14%\n",
            "iter 47: loss 6.7584, time 12910.21ms, mfu 5.15%\n",
            "iter 48: loss 6.5084, time 13001.62ms, mfu 5.15%\n",
            "iter 49: loss 6.5082, time 13048.10ms, mfu 5.15%\n",
            "iter 50: loss 6.5509, time 13146.84ms, mfu 5.15%\n",
            "iter 51: loss 6.6599, time 13125.51ms, mfu 5.15%\n",
            "iter 52: loss 5.8890, time 13100.41ms, mfu 5.15%\n",
            "iter 53: loss 6.3400, time 13112.51ms, mfu 5.14%\n",
            "iter 54: loss 6.4964, time 13192.69ms, mfu 5.14%\n",
            "iter 55: loss 6.0329, time 13240.79ms, mfu 5.13%\n",
            "iter 56: loss 6.3362, time 13231.78ms, mfu 5.13%\n",
            "iter 57: loss 6.4144, time 13210.24ms, mfu 5.13%\n",
            "iter 58: loss 6.3990, time 13233.96ms, mfu 5.12%\n",
            "iter 59: loss 6.5405, time 13232.40ms, mfu 5.12%\n",
            "iter 60: loss 6.0539, time 13292.27ms, mfu 5.11%\n",
            "iter 61: loss 6.4059, time 13261.81ms, mfu 5.11%\n",
            "iter 62: loss 6.0674, time 13277.48ms, mfu 5.11%\n",
            "iter 63: loss 5.7154, time 13279.47ms, mfu 5.10%\n",
            "step 64: train loss 6.0813, val loss 7.1812\n",
            "saving checkpoint to drive/MyDrive/model_out\n",
            "iter 64: loss 6.3606, time 23934.59ms, mfu 4.87%\n",
            "iter 65: loss 6.2865, time 13673.33ms, mfu 4.88%\n",
            "iter 66: loss 5.8924, time 13616.85ms, mfu 4.89%\n",
            "iter 67: loss 6.3669, time 13410.55ms, mfu 4.90%\n",
            "iter 68: loss 5.7833, time 13253.23ms, mfu 4.92%\n",
            "iter 69: loss 6.1294, time 13161.87ms, mfu 4.94%\n",
            "iter 70: loss 6.1996, time 13184.24ms, mfu 4.95%\n",
            "iter 71: loss 5.9037, time 13264.93ms, mfu 4.97%\n",
            "iter 72: loss 5.6360, time 13391.14ms, mfu 4.97%\n",
            "iter 73: loss 6.0901, time 13377.68ms, mfu 4.98%\n",
            "iter 74: loss 6.1177, time 13375.62ms, mfu 4.98%\n",
            "iter 75: loss 5.9573, time 13366.85ms, mfu 4.99%\n",
            "iter 76: loss 5.7875, time 13395.59ms, mfu 4.99%\n",
            "iter 77: loss 6.1013, time 13390.15ms, mfu 5.00%\n",
            "iter 78: loss 6.1094, time 13412.80ms, mfu 5.00%\n",
            "iter 79: loss 5.6475, time 13408.38ms, mfu 5.00%\n",
            "iter 80: loss 6.0277, time 13375.92ms, mfu 5.00%\n",
            "iter 81: loss 5.9584, time 13372.00ms, mfu 5.01%\n",
            "iter 82: loss 6.0447, time 13409.24ms, mfu 5.01%\n",
            "iter 83: loss 6.0896, time 13385.35ms, mfu 5.01%\n",
            "iter 84: loss 5.6594, time 13388.47ms, mfu 5.01%\n",
            "iter 85: loss 6.0284, time 13449.37ms, mfu 5.01%\n",
            "iter 86: loss 5.8225, time 13437.61ms, mfu 5.01%\n",
            "iter 87: loss 5.4645, time 13441.32ms, mfu 5.01%\n",
            "iter 88: loss 5.8133, time 13443.12ms, mfu 5.01%\n",
            "iter 89: loss 5.7693, time 13416.06ms, mfu 5.01%\n",
            "iter 90: loss 5.7507, time 13433.70ms, mfu 5.01%\n",
            "iter 91: loss 6.0440, time 13453.32ms, mfu 5.01%\n",
            "iter 92: loss 6.0496, time 13487.47ms, mfu 5.01%\n",
            "iter 93: loss 5.8816, time 13436.26ms, mfu 5.01%\n",
            "iter 94: loss 5.9930, time 13431.52ms, mfu 5.01%\n",
            "iter 95: loss 5.8606, time 13411.56ms, mfu 5.01%\n",
            "step 96: train loss 5.6077, val loss 7.0921\n",
            "saving checkpoint to drive/MyDrive/model_out\n",
            "iter 96: loss 5.7051, time 20342.18ms, mfu 4.84%\n",
            "iter 97: loss 5.7566, time 13619.06ms, mfu 4.85%\n",
            "iter 98: loss 5.5271, time 13683.98ms, mfu 4.86%\n",
            "iter 99: loss 5.9663, time 13571.29ms, mfu 4.87%\n",
            "iter 100: loss 5.6317, time 13413.07ms, mfu 4.88%\n",
            "iter 101: loss 5.7661, time 13384.39ms, mfu 4.90%\n",
            "iter 102: loss 5.7995, time 13397.20ms, mfu 4.91%\n",
            "iter 103: loss 5.7366, time 13431.29ms, mfu 4.92%\n",
            "iter 104: loss 5.6115, time 13426.52ms, mfu 4.93%\n",
            "iter 105: loss 5.4933, time 13481.69ms, mfu 4.94%\n",
            "iter 106: loss 5.7257, time 13522.41ms, mfu 4.94%\n",
            "iter 107: loss 5.5246, time 13459.76ms, mfu 4.95%\n",
            "iter 108: loss 5.6939, time 13481.69ms, mfu 4.95%\n",
            "iter 109: loss 5.6162, time 13530.18ms, mfu 4.95%\n",
            "iter 110: loss 5.5081, time 13530.44ms, mfu 4.96%\n",
            "iter 111: loss 5.6594, time 13501.34ms, mfu 4.96%\n",
            "iter 112: loss 5.4878, time 13515.25ms, mfu 4.96%\n",
            "iter 113: loss 5.5094, time 13522.89ms, mfu 4.96%\n",
            "iter 114: loss 5.3439, time 13488.77ms, mfu 4.97%\n",
            "iter 115: loss 5.2483, time 13514.00ms, mfu 4.97%\n",
            "iter 116: loss 4.7967, time 13508.09ms, mfu 4.97%\n",
            "iter 117: loss 5.2566, time 13521.94ms, mfu 4.97%\n",
            "iter 118: loss 5.4597, time 13504.83ms, mfu 4.97%\n",
            "iter 119: loss 5.2066, time 13486.52ms, mfu 4.97%\n",
            "iter 120: loss 5.5946, time 13500.46ms, mfu 4.98%\n",
            "iter 121: loss 4.9217, time 13486.29ms, mfu 4.98%\n",
            "iter 122: loss 5.2031, time 13480.67ms, mfu 4.98%\n",
            "iter 123: loss 5.4139, time 13460.88ms, mfu 4.98%\n",
            "iter 124: loss 4.6008, time 13493.68ms, mfu 4.98%\n",
            "iter 125: loss 5.3353, time 13494.17ms, mfu 4.98%\n",
            "iter 126: loss 4.7616, time 13509.53ms, mfu 4.98%\n",
            "iter 127: loss 4.5991, time 13464.71ms, mfu 4.98%\n",
            "step 128: train loss 5.1236, val loss 7.0626\n",
            "saving checkpoint to drive/MyDrive/model_out\n",
            "iter 128: loss 5.1432, time 30314.84ms, mfu 4.71%\n",
            "iter 129: loss 4.6920, time 14079.00ms, mfu 4.72%\n",
            "iter 130: loss 5.1949, time 14194.60ms, mfu 4.72%\n",
            "iter 131: loss 4.9222, time 13696.47ms, mfu 4.74%\n",
            "iter 132: loss 4.9253, time 13331.86ms, mfu 4.77%\n",
            "iter 133: loss 5.0176, time 13171.79ms, mfu 4.80%\n",
            "iter 134: loss 5.2078, time 13288.23ms, mfu 4.83%\n",
            "iter 135: loss 4.9970, time 13462.21ms, mfu 4.85%\n",
            "iter 136: loss 5.0425, time 13626.43ms, mfu 4.86%\n",
            "iter 137: loss 4.5814, time 13625.39ms, mfu 4.86%\n",
            "iter 138: loss 5.1337, time 13544.14ms, mfu 4.88%\n",
            "iter 139: loss 4.9197, time 13465.17ms, mfu 4.89%\n",
            "iter 140: loss 4.9742, time 13413.66ms, mfu 4.90%\n",
            "iter 141: loss 4.9259, time 13419.33ms, mfu 4.91%\n",
            "iter 142: loss 4.8995, time 13440.51ms, mfu 4.92%\n",
            "iter 143: loss 4.5560, time 13466.48ms, mfu 4.93%\n",
            "iter 144: loss 4.1215, time 13498.39ms, mfu 4.94%\n",
            "iter 145: loss 4.8255, time 13510.66ms, mfu 4.94%\n",
            "iter 146: loss 4.4737, time 13538.68ms, mfu 4.94%\n",
            "iter 147: loss 4.2321, time 13541.76ms, mfu 4.95%\n",
            "iter 148: loss 3.8748, time 13582.92ms, mfu 4.95%\n",
            "iter 149: loss 4.6759, time 13615.96ms, mfu 4.95%\n",
            "iter 150: loss 4.2361, time 13563.24ms, mfu 4.95%\n",
            "iter 151: loss 4.6991, time 13534.12ms, mfu 4.95%\n",
            "iter 152: loss 4.1468, time 13502.05ms, mfu 4.96%\n",
            "iter 153: loss 4.7880, time 13519.23ms, mfu 4.96%\n",
            "iter 154: loss 4.1623, time 13517.88ms, mfu 4.96%\n",
            "iter 155: loss 4.4420, time 13470.89ms, mfu 4.96%\n",
            "iter 156: loss 4.7236, time 13457.25ms, mfu 4.97%\n",
            "iter 157: loss 4.4772, time 13442.39ms, mfu 4.97%\n",
            "iter 158: loss 4.1870, time 13472.29ms, mfu 4.97%\n",
            "iter 159: loss 4.7575, time 13469.50ms, mfu 4.98%\n",
            "step 160: train loss 4.6364, val loss 6.5913\n",
            "saving checkpoint to drive/MyDrive/model_out\n",
            "iter 160: loss 3.5429, time 21959.34ms, mfu 4.79%\n",
            "iter 161: loss 4.4008, time 13769.30ms, mfu 4.80%\n",
            "iter 162: loss 4.4154, time 13885.85ms, mfu 4.80%\n",
            "iter 163: loss 4.6445, time 13623.81ms, mfu 4.82%\n",
            "iter 164: loss 4.6641, time 13460.24ms, mfu 4.83%\n",
            "iter 165: loss 4.5488, time 13403.90ms, mfu 4.85%\n",
            "iter 166: loss 4.1486, time 13436.92ms, mfu 4.87%\n",
            "iter 167: loss 4.6914, time 13489.83ms, mfu 4.88%\n",
            "iter 168: loss 3.9821, time 13538.75ms, mfu 4.89%\n",
            "iter 169: loss 4.6940, time 13560.88ms, mfu 4.90%\n",
            "iter 170: loss 4.4294, time 13571.16ms, mfu 4.90%\n",
            "iter 171: loss 4.4630, time 13563.88ms, mfu 4.91%\n",
            "iter 172: loss 4.4338, time 13583.61ms, mfu 4.91%\n",
            "iter 173: loss 4.4261, time 13598.81ms, mfu 4.92%\n",
            "iter 174: loss 4.1065, time 13554.52ms, mfu 4.92%\n",
            "iter 175: loss 4.4443, time 13618.62ms, mfu 4.93%\n",
            "iter 176: loss 4.2222, time 13622.31ms, mfu 4.93%\n",
            "iter 177: loss 4.5940, time 13566.51ms, mfu 4.93%\n",
            "iter 178: loss 3.9463, time 13565.64ms, mfu 4.93%\n",
            "iter 179: loss 3.2438, time 13596.39ms, mfu 4.94%\n",
            "iter 180: loss 4.4017, time 13602.25ms, mfu 4.94%\n",
            "iter 181: loss 4.2557, time 13555.52ms, mfu 4.94%\n",
            "iter 182: loss 3.7236, time 13567.32ms, mfu 4.94%\n",
            "iter 183: loss 4.4786, time 13595.93ms, mfu 4.94%\n",
            "iter 184: loss 3.6416, time 13557.45ms, mfu 4.95%\n",
            "iter 185: loss 3.8184, time 13537.59ms, mfu 4.95%\n",
            "iter 186: loss 4.2819, time 13545.94ms, mfu 4.95%\n",
            "iter 187: loss 3.5755, time 13567.78ms, mfu 4.95%\n",
            "iter 188: loss 4.1126, time 13573.26ms, mfu 4.95%\n",
            "iter 189: loss 3.6597, time 13553.54ms, mfu 4.95%\n",
            "iter 190: loss 4.2518, time 13561.91ms, mfu 4.96%\n",
            "iter 191: loss 3.9268, time 13578.78ms, mfu 4.96%\n",
            "step 192: train loss 4.1070, val loss 6.7950\n",
            "saving checkpoint to drive/MyDrive/model_out\n",
            "iter 192: loss 3.3392, time 23117.18ms, mfu 4.75%\n",
            "iter 193: loss 3.5897, time 13838.75ms, mfu 4.76%\n",
            "iter 194: loss 4.0286, time 13870.27ms, mfu 4.77%\n",
            "iter 195: loss 4.0630, time 13640.69ms, mfu 4.79%\n",
            "iter 196: loss 4.0438, time 13450.90ms, mfu 4.81%\n",
            "iter 197: loss 4.1419, time 13430.28ms, mfu 4.83%\n",
            "iter 198: loss 3.8943, time 13498.53ms, mfu 4.85%\n",
            "iter 199: loss 4.1283, time 13555.40ms, mfu 4.86%\n",
            "iter 200: loss 3.7582, time 13580.92ms, mfu 4.87%\n",
            "iter 201: loss 3.1825, time 13583.01ms, mfu 4.88%\n",
            "iter 202: loss 3.8618, time 13628.66ms, mfu 4.88%\n",
            "iter 203: loss 4.0526, time 13627.05ms, mfu 4.89%\n",
            "iter 204: loss 4.1436, time 13587.67ms, mfu 4.90%\n",
            "iter 205: loss 3.8927, time 13566.29ms, mfu 4.90%\n",
            "iter 206: loss 3.9582, time 13606.11ms, mfu 4.91%\n",
            "iter 207: loss 3.4298, time 13596.42ms, mfu 4.91%\n",
            "iter 208: loss 3.9250, time 13608.38ms, mfu 4.92%\n",
            "iter 209: loss 3.8647, time 13583.42ms, mfu 4.92%\n",
            "iter 210: loss 4.0295, time 13593.79ms, mfu 4.92%\n",
            "iter 211: loss 3.7608, time 13622.70ms, mfu 4.92%\n",
            "iter 212: loss 3.6593, time 13577.60ms, mfu 4.93%\n",
            "iter 213: loss 3.7787, time 13556.07ms, mfu 4.93%\n",
            "iter 214: loss 3.4268, time 13589.79ms, mfu 4.93%\n",
            "iter 215: loss 3.4625, time 13568.58ms, mfu 4.94%\n",
            "iter 216: loss 3.3016, time 13547.81ms, mfu 4.94%\n",
            "iter 217: loss 3.2940, time 13513.14ms, mfu 4.94%\n",
            "iter 218: loss 3.6882, time 13541.56ms, mfu 4.95%\n",
            "iter 219: loss 3.3235, time 13515.03ms, mfu 4.95%\n",
            "iter 220: loss 3.4657, time 13488.76ms, mfu 4.95%\n",
            "iter 221: loss 3.4881, time 13496.64ms, mfu 4.96%\n",
            "iter 222: loss 3.4829, time 13507.15ms, mfu 4.96%\n",
            "iter 223: loss 3.4929, time 13506.73ms, mfu 4.96%\n",
            "step 224: train loss 3.2376, val loss 6.4873\n",
            "saving checkpoint to drive/MyDrive/model_out\n",
            "iter 224: loss 3.6456, time 21821.65ms, mfu 4.78%\n",
            "iter 225: loss 3.7076, time 13774.77ms, mfu 4.79%\n",
            "iter 226: loss 3.6562, time 13895.27ms, mfu 4.79%\n",
            "iter 227: loss 3.4549, time 13701.24ms, mfu 4.80%\n",
            "iter 228: loss 3.5635, time 13576.45ms, mfu 4.82%\n",
            "iter 229: loss 3.5567, time 13439.07ms, mfu 4.84%\n",
            "iter 230: loss 3.5444, time 13424.31ms, mfu 4.86%\n",
            "iter 231: loss 3.1387, time 13543.12ms, mfu 4.87%\n",
            "iter 232: loss 3.5050, time 13639.27ms, mfu 4.88%\n",
            "iter 233: loss 3.4676, time 13648.69ms, mfu 4.88%\n",
            "iter 234: loss 2.5040, time 13573.99ms, mfu 4.89%\n",
            "iter 235: loss 3.2407, time 13529.09ms, mfu 4.90%\n",
            "iter 236: loss 3.1089, time 13510.08ms, mfu 4.91%\n",
            "iter 237: loss 3.0740, time 13510.55ms, mfu 4.91%\n",
            "iter 238: loss 3.2902, time 13495.89ms, mfu 4.92%\n",
            "iter 239: loss 3.3296, time 13480.04ms, mfu 4.93%\n",
            "iter 240: loss 3.3319, time 13479.38ms, mfu 4.94%\n",
            "iter 241: loss 3.0057, time 13481.73ms, mfu 4.94%\n",
            "iter 242: loss 3.2482, time 13523.89ms, mfu 4.94%\n",
            "iter 243: loss 3.0900, time 13496.54ms, mfu 4.95%\n",
            "iter 244: loss 3.4201, time 13473.38ms, mfu 4.95%\n",
            "iter 245: loss 3.0697, time 13533.38ms, mfu 4.96%\n",
            "iter 246: loss 2.7802, time 13586.28ms, mfu 4.96%\n",
            "iter 247: loss 2.8525, time 13597.32ms, mfu 4.96%\n",
            "iter 248: loss 3.2269, time 13590.31ms, mfu 4.96%\n",
            "iter 249: loss 3.4471, time 13593.80ms, mfu 4.96%\n",
            "iter 250: loss 2.7038, time 13576.28ms, mfu 4.96%\n",
            "iter 251: loss 3.1421, time 13580.88ms, mfu 4.96%\n",
            "iter 252: loss 2.8871, time 13614.73ms, mfu 4.95%\n",
            "iter 253: loss 3.1394, time 13562.96ms, mfu 4.96%\n",
            "iter 254: loss 3.0634, time 13598.95ms, mfu 4.96%\n",
            "iter 255: loss 2.8360, time 13576.17ms, mfu 4.96%\n",
            "step 256: train loss 2.8933, val loss 6.7612\n",
            "saving checkpoint to drive/MyDrive/model_out\n",
            "iter 256: loss 3.0272, time 31727.33ms, mfu 4.67%\n",
            "iter 257: loss 3.1197, time 14211.66ms, mfu 4.68%\n",
            "iter 258: loss 3.0823, time 14357.77ms, mfu 4.68%\n",
            "iter 259: loss 2.8627, time 13738.23ms, mfu 4.70%\n",
            "iter 260: loss 3.1973, time 13375.50ms, mfu 4.74%\n",
            "iter 261: loss 3.0051, time 13253.58ms, mfu 4.77%\n",
            "iter 262: loss 2.8403, time 13320.74ms, mfu 4.80%\n",
            "iter 263: loss 2.7576, time 13506.11ms, mfu 4.82%\n",
            "iter 264: loss 3.2189, time 13698.40ms, mfu 4.83%\n",
            "iter 265: loss 2.6274, time 13692.06ms, mfu 4.84%\n",
            "iter 266: loss 2.6166, time 13614.92ms, mfu 4.85%\n",
            "iter 267: loss 2.9681, time 13520.74ms, mfu 4.86%\n",
            "iter 268: loss 3.0536, time 13481.36ms, mfu 4.87%\n",
            "iter 269: loss 2.8294, time 13495.83ms, mfu 4.88%\n",
            "iter 270: loss 2.8342, time 13520.17ms, mfu 4.89%\n",
            "iter 271: loss 2.8832, time 13543.61ms, mfu 4.90%\n",
            "iter 272: loss 2.5020, time 13606.00ms, mfu 4.91%\n",
            "iter 273: loss 2.4904, time 13616.94ms, mfu 4.91%\n",
            "iter 274: loss 2.7226, time 13630.25ms, mfu 4.91%\n",
            "iter 275: loss 2.6871, time 13611.62ms, mfu 4.92%\n",
            "iter 276: loss 2.6915, time 13644.18ms, mfu 4.92%\n",
            "iter 277: loss 2.5258, time 13594.72ms, mfu 4.92%\n",
            "iter 278: loss 2.9762, time 13641.45ms, mfu 4.92%\n",
            "iter 279: loss 2.6810, time 13669.66ms, mfu 4.92%\n",
            "iter 280: loss 2.3583, time 13582.63ms, mfu 4.93%\n",
            "iter 281: loss 2.5837, time 13531.98ms, mfu 4.93%\n",
            "iter 282: loss 2.7247, time 13539.84ms, mfu 4.94%\n",
            "iter 283: loss 2.5720, time 13511.79ms, mfu 4.94%\n",
            "iter 284: loss 2.5562, time 13516.98ms, mfu 4.94%\n",
            "iter 285: loss 2.3849, time 13537.51ms, mfu 4.95%\n",
            "iter 286: loss 2.6897, time 13582.43ms, mfu 4.95%\n",
            "iter 287: loss 2.6010, time 13538.85ms, mfu 4.95%\n",
            "step 288: train loss 2.5268, val loss 7.0453\n",
            "saving checkpoint to drive/MyDrive/model_out\n",
            "iter 288: loss 2.5438, time 24985.16ms, mfu 4.73%\n",
            "iter 289: loss 2.3241, time 14010.03ms, mfu 4.73%\n",
            "iter 290: loss 2.6598, time 14022.22ms, mfu 4.74%\n",
            "iter 291: loss 2.6195, time 13671.24ms, mfu 4.76%\n",
            "iter 292: loss 2.4566, time 13464.10ms, mfu 4.78%\n",
            "iter 293: loss 2.3504, time 13391.30ms, mfu 4.81%\n",
            "iter 294: loss 2.6032, time 13464.87ms, mfu 4.83%\n",
            "iter 295: loss 2.6325, time 13584.57ms, mfu 4.84%\n",
            "iter 296: loss 2.2566, time 13672.64ms, mfu 4.85%\n",
            "iter 297: loss 2.4787, time 13643.00ms, mfu 4.86%\n",
            "iter 298: loss 2.5291, time 13553.27ms, mfu 4.87%\n",
            "iter 299: loss 2.1102, time 13556.57ms, mfu 4.88%\n",
            "iter 300: loss 2.5165, time 13512.72ms, mfu 4.89%\n",
            "iter 301: loss 2.5144, time 13531.69ms, mfu 4.90%\n",
            "iter 302: loss 2.6329, time 13506.83ms, mfu 4.91%\n",
            "iter 303: loss 2.4661, time 13559.96ms, mfu 4.91%\n",
            "iter 304: loss 2.5097, time 13601.15ms, mfu 4.92%\n",
            "iter 305: loss 2.5586, time 13583.24ms, mfu 4.92%\n",
            "iter 306: loss 2.4289, time 13582.51ms, mfu 4.92%\n",
            "iter 307: loss 2.5366, time 13582.72ms, mfu 4.93%\n",
            "iter 308: loss 2.6051, time 13612.81ms, mfu 4.93%\n",
            "iter 309: loss 2.2313, time 13583.36ms, mfu 4.93%\n",
            "iter 310: loss 2.2688, time 13551.69ms, mfu 4.94%\n",
            "iter 311: loss 2.2425, time 13574.06ms, mfu 4.94%\n",
            "iter 312: loss 2.2476, time 13549.43ms, mfu 4.94%\n",
            "iter 313: loss 2.1026, time 13564.71ms, mfu 4.94%\n",
            "iter 314: loss 2.4007, time 13570.18ms, mfu 4.94%\n",
            "iter 315: loss 2.1573, time 13558.23ms, mfu 4.95%\n",
            "iter 316: loss 2.2090, time 13539.12ms, mfu 4.95%\n",
            "iter 317: loss 2.3037, time 13525.43ms, mfu 4.95%\n",
            "iter 318: loss 2.2846, time 13524.81ms, mfu 4.96%\n",
            "iter 319: loss 2.0389, time 13534.48ms, mfu 4.96%\n",
            "step 320: train loss 2.3019, val loss 6.5502\n",
            "saving checkpoint to drive/MyDrive/model_out\n",
            "iter 320: loss 2.2890, time 26550.58ms, mfu 4.71%\n",
            "iter 321: loss 2.2221, time 14043.15ms, mfu 4.72%\n",
            "iter 322: loss 2.3876, time 14017.13ms, mfu 4.73%\n",
            "iter 323: loss 2.1021, time 13674.14ms, mfu 4.75%\n",
            "iter 324: loss 2.1987, time 13419.37ms, mfu 4.78%\n",
            "iter 325: loss 1.7541, time 13333.26ms, mfu 4.80%\n",
            "iter 326: loss 2.0252, time 13389.83ms, mfu 4.83%\n",
            "iter 327: loss 2.0641, time 13548.22ms, mfu 4.84%\n",
            "iter 328: loss 2.0292, time 13619.42ms, mfu 4.85%\n",
            "iter 329: loss 2.3033, time 13688.26ms, mfu 4.86%\n",
            "iter 330: loss 1.9351, time 13610.93ms, mfu 4.87%\n",
            "iter 331: loss 2.0434, time 13575.73ms, mfu 4.88%\n",
            "iter 332: loss 2.2514, time 13597.52ms, mfu 4.88%\n",
            "iter 333: loss 2.0024, time 13553.26ms, mfu 4.89%\n",
            "iter 334: loss 1.6233, time 13542.99ms, mfu 4.90%\n",
            "iter 335: loss 1.9023, time 13575.04ms, mfu 4.91%\n",
            "iter 336: loss 2.1124, time 13585.78ms, mfu 4.91%\n",
            "iter 337: loss 2.1331, time 13556.02ms, mfu 4.92%\n",
            "iter 338: loss 2.0782, time 13560.42ms, mfu 4.92%\n",
            "iter 339: loss 2.1005, time 13550.64ms, mfu 4.93%\n",
            "iter 340: loss 1.9192, time 13546.67ms, mfu 4.93%\n",
            "iter 341: loss 2.1019, time 13487.17ms, mfu 4.94%\n",
            "iter 342: loss 2.0851, time 13531.31ms, mfu 4.94%\n",
            "iter 343: loss 2.0757, time 13551.70ms, mfu 4.94%\n",
            "iter 344: loss 1.7803, time 13543.97ms, mfu 4.95%\n",
            "iter 345: loss 2.0955, time 13555.72ms, mfu 4.95%\n",
            "iter 346: loss 1.9009, time 13560.08ms, mfu 4.95%\n",
            "iter 347: loss 1.9475, time 13543.55ms, mfu 4.95%\n",
            "iter 348: loss 1.8743, time 13556.90ms, mfu 4.95%\n",
            "iter 349: loss 1.8395, time 13586.57ms, mfu 4.95%\n",
            "iter 350: loss 1.6996, time 13547.08ms, mfu 4.96%\n",
            "iter 351: loss 1.9974, time 13566.65ms, mfu 4.96%\n",
            "step 352: train loss 1.8607, val loss 6.4998\n",
            "saving checkpoint to drive/MyDrive/model_out\n",
            "iter 352: loss 1.7910, time 20865.51ms, mfu 4.78%\n",
            "iter 353: loss 1.9366, time 13762.56ms, mfu 4.79%\n",
            "iter 354: loss 1.9799, time 13794.14ms, mfu 4.80%\n",
            "iter 355: loss 1.9136, time 13690.06ms, mfu 4.81%\n",
            "iter 356: loss 1.8988, time 13551.43ms, mfu 4.83%\n",
            "iter 357: loss 1.7602, time 13473.42ms, mfu 4.85%\n",
            "iter 358: loss 1.9012, time 13494.87ms, mfu 4.86%\n",
            "iter 359: loss 1.9365, time 13548.68ms, mfu 4.87%\n",
            "iter 360: loss 1.9311, time 13568.08ms, mfu 4.88%\n",
            "iter 361: loss 1.8985, time 13581.58ms, mfu 4.89%\n",
            "iter 362: loss 1.6896, time 13613.23ms, mfu 4.89%\n",
            "iter 363: loss 1.7769, time 13595.83ms, mfu 4.90%\n",
            "iter 364: loss 1.8076, time 13602.80ms, mfu 4.90%\n",
            "iter 365: loss 1.7283, time 13598.21ms, mfu 4.91%\n",
            "iter 366: loss 1.6249, time 13571.42ms, mfu 4.91%\n",
            "iter 367: loss 1.7335, time 13583.57ms, mfu 4.92%\n",
            "iter 368: loss 1.4427, time 13582.16ms, mfu 4.92%\n",
            "iter 369: loss 1.5711, time 13566.55ms, mfu 4.93%\n",
            "iter 370: loss 1.8090, time 13587.60ms, mfu 4.93%\n",
            "iter 371: loss 1.8754, time 13614.52ms, mfu 4.93%\n",
            "iter 372: loss 1.7205, time 13616.82ms, mfu 4.93%\n",
            "iter 373: loss 1.7267, time 13629.82ms, mfu 4.93%\n",
            "iter 374: loss 1.7042, time 13581.54ms, mfu 4.94%\n",
            "iter 375: loss 1.5529, time 13554.94ms, mfu 4.94%\n",
            "iter 376: loss 1.6419, time 13569.06ms, mfu 4.94%\n",
            "iter 377: loss 1.7976, time 13585.76ms, mfu 4.94%\n",
            "iter 378: loss 1.6272, time 13651.52ms, mfu 4.94%\n",
            "iter 379: loss 1.7234, time 13649.64ms, mfu 4.94%\n",
            "iter 380: loss 1.5856, time 13595.63ms, mfu 4.94%\n",
            "iter 381: loss 1.6797, time 13548.53ms, mfu 4.94%\n",
            "iter 382: loss 1.5292, time 13602.98ms, mfu 4.95%\n",
            "iter 383: loss 1.5326, time 13600.47ms, mfu 4.95%\n",
            "step 384: train loss 1.6363, val loss 7.0167\n",
            "saving checkpoint to drive/MyDrive/model_out\n",
            "iter 384: loss 1.5983, time 21124.20ms, mfu 4.77%\n",
            "iter 385: loss 1.6103, time 13782.83ms, mfu 4.78%\n",
            "iter 386: loss 1.7448, time 13819.61ms, mfu 4.79%\n",
            "iter 387: loss 1.7865, time 13675.62ms, mfu 4.80%\n",
            "iter 388: loss 1.4631, time 13527.71ms, mfu 4.82%\n",
            "iter 389: loss 1.4868, time 13486.49ms, mfu 4.84%\n",
            "iter 390: loss 1.5527, time 13524.81ms, mfu 4.85%\n",
            "iter 391: loss 1.6073, time 13544.47ms, mfu 4.86%\n",
            "iter 392: loss 1.4894, time 13593.59ms, mfu 4.87%\n",
            "iter 393: loss 1.5765, time 13616.60ms, mfu 4.88%\n",
            "iter 394: loss 1.6231, time 13667.80ms, mfu 4.88%\n",
            "iter 395: loss 1.5092, time 13648.42ms, mfu 4.89%\n",
            "iter 396: loss 1.4415, time 13666.57ms, mfu 4.89%\n",
            "iter 397: loss 1.3701, time 13636.92ms, mfu 4.90%\n",
            "iter 398: loss 1.5434, time 13658.23ms, mfu 4.90%\n",
            "iter 399: loss 1.5996, time 13649.65ms, mfu 4.90%\n",
            "iter 400: loss 1.6109, time 13640.97ms, mfu 4.91%\n",
            "iter 401: loss 1.6189, time 13625.59ms, mfu 4.91%\n",
            "iter 402: loss 1.4069, time 13661.16ms, mfu 4.91%\n",
            "iter 403: loss 1.3392, time 13657.29ms, mfu 4.91%\n",
            "iter 404: loss 1.5840, time 13664.88ms, mfu 4.92%\n",
            "iter 405: loss 1.5994, time 13660.92ms, mfu 4.92%\n",
            "iter 406: loss 1.1335, time 13642.24ms, mfu 4.92%\n",
            "iter 407: loss 1.5324, time 13592.37ms, mfu 4.92%\n",
            "iter 408: loss 1.4567, time 13617.16ms, mfu 4.92%\n",
            "iter 409: loss 1.4869, time 13590.87ms, mfu 4.93%\n",
            "iter 410: loss 1.2477, time 13595.85ms, mfu 4.93%\n",
            "iter 411: loss 1.4842, time 13615.97ms, mfu 4.93%\n",
            "iter 412: loss 1.3075, time 13635.27ms, mfu 4.93%\n",
            "iter 413: loss 1.3080, time 13641.11ms, mfu 4.93%\n",
            "iter 414: loss 1.2591, time 13621.83ms, mfu 4.93%\n",
            "iter 415: loss 1.2846, time 13615.07ms, mfu 4.93%\n",
            "step 416: train loss 1.4335, val loss 6.8853\n",
            "saving checkpoint to drive/MyDrive/model_out\n",
            "iter 416: loss 1.4138, time 23453.99ms, mfu 4.73%\n",
            "iter 417: loss 1.5504, time 13845.10ms, mfu 4.74%\n",
            "iter 418: loss 1.5475, time 13946.79ms, mfu 4.75%\n",
            "iter 419: loss 1.3182, time 13739.22ms, mfu 4.77%\n",
            "iter 420: loss 1.5355, time 13517.55ms, mfu 4.79%\n",
            "iter 421: loss 1.4115, time 13412.54ms, mfu 4.81%\n",
            "iter 422: loss 1.7782, time 13423.78ms, mfu 4.83%\n",
            "iter 423: loss 1.4940, time 13541.21ms, mfu 4.84%\n",
            "iter 424: loss 1.3525, time 13663.95ms, mfu 4.85%\n",
            "iter 425: loss 1.4361, time 13665.93ms, mfu 4.86%\n",
            "iter 426: loss 1.1991, time 13631.95ms, mfu 4.87%\n",
            "iter 427: loss 1.3075, time 13543.60ms, mfu 4.88%\n",
            "iter 428: loss 1.5339, time 13526.45ms, mfu 4.89%\n",
            "iter 429: loss 1.3896, time 13515.45ms, mfu 4.90%\n",
            "iter 430: loss 1.4784, time 13563.81ms, mfu 4.90%\n",
            "iter 431: loss 1.4039, time 13600.68ms, mfu 4.91%\n",
            "iter 432: loss 1.3441, time 13603.33ms, mfu 4.91%\n",
            "iter 433: loss 1.4427, time 13576.19ms, mfu 4.92%\n",
            "iter 434: loss 1.3760, time 13571.95ms, mfu 4.92%\n",
            "iter 435: loss 1.3733, time 13556.34ms, mfu 4.93%\n",
            "iter 436: loss 1.3970, time 13573.53ms, mfu 4.93%\n",
            "iter 437: loss 1.2867, time 13563.18ms, mfu 4.93%\n",
            "iter 438: loss 1.2505, time 13568.03ms, mfu 4.94%\n",
            "iter 439: loss 1.3657, time 13559.92ms, mfu 4.94%\n",
            "iter 440: loss 1.2529, time 13601.98ms, mfu 4.94%\n",
            "iter 441: loss 1.3426, time 13578.40ms, mfu 4.94%\n",
            "iter 442: loss 1.2897, time 13556.42ms, mfu 4.94%\n",
            "iter 443: loss 1.3473, time 13612.95ms, mfu 4.94%\n",
            "iter 444: loss 1.2854, time 13597.27ms, mfu 4.95%\n",
            "iter 445: loss 1.0478, time 13641.02ms, mfu 4.94%\n",
            "iter 446: loss 1.2024, time 13629.87ms, mfu 4.94%\n",
            "iter 447: loss 1.3182, time 13605.72ms, mfu 4.94%\n",
            "step 448: train loss 1.3304, val loss 7.1242\n",
            "saving checkpoint to drive/MyDrive/model_out\n",
            "iter 448: loss 1.2836, time 20309.31ms, mfu 4.78%\n",
            "iter 449: loss 1.2010, time 13687.53ms, mfu 4.80%\n",
            "iter 450: loss 1.3407, time 13801.23ms, mfu 4.80%\n",
            "iter 451: loss 1.2840, time 13719.48ms, mfu 4.81%\n",
            "iter 452: loss 1.2514, time 13607.65ms, mfu 4.83%\n",
            "iter 453: loss 1.1473, time 13493.52ms, mfu 4.84%\n",
            "iter 454: loss 1.1875, time 13493.72ms, mfu 4.86%\n",
            "iter 455: loss 1.2740, time 13576.08ms, mfu 4.87%\n",
            "iter 456: loss 1.2152, time 13619.04ms, mfu 4.88%\n",
            "iter 457: loss 1.2613, time 13664.46ms, mfu 4.88%\n",
            "iter 458: loss 1.1010, time 13677.70ms, mfu 4.89%\n",
            "iter 459: loss 0.9361, time 13671.55ms, mfu 4.89%\n",
            "iter 460: loss 1.1736, time 13632.91ms, mfu 4.89%\n",
            "iter 461: loss 1.3468, time 13662.17ms, mfu 4.90%\n",
            "iter 462: loss 1.2441, time 13662.75ms, mfu 4.90%\n",
            "iter 463: loss 1.3106, time 13647.58ms, mfu 4.90%\n",
            "iter 464: loss 1.3437, time 13623.89ms, mfu 4.91%\n",
            "iter 465: loss 1.1962, time 13655.33ms, mfu 4.91%\n",
            "iter 466: loss 1.2083, time 13644.43ms, mfu 4.91%\n",
            "iter 467: loss 1.2464, time 13597.17ms, mfu 4.92%\n",
            "iter 468: loss 1.3434, time 13617.42ms, mfu 4.92%\n",
            "iter 469: loss 1.3044, time 13594.98ms, mfu 4.92%\n",
            "iter 470: loss 1.2185, time 13653.25ms, mfu 4.92%\n",
            "iter 471: loss 1.1814, time 13638.70ms, mfu 4.92%\n",
            "iter 472: loss 1.2440, time 13658.22ms, mfu 4.93%\n",
            "iter 473: loss 1.2519, time 13642.85ms, mfu 4.93%\n",
            "iter 474: loss 1.1277, time 13638.76ms, mfu 4.93%\n",
            "iter 475: loss 1.3432, time 13641.76ms, mfu 4.93%\n",
            "iter 476: loss 1.2995, time 13670.03ms, mfu 4.93%\n",
            "iter 477: loss 1.2583, time 13576.22ms, mfu 4.93%\n",
            "iter 478: loss 1.0528, time 13600.49ms, mfu 4.93%\n",
            "iter 479: loss 1.1500, time 13580.31ms, mfu 4.94%\n",
            "step 480: train loss 1.1829, val loss 6.2973\n",
            "saving checkpoint to drive/MyDrive/model_out\n",
            "iter 480: loss 1.0860, time 20287.15ms, mfu 4.77%\n",
            "iter 481: loss 1.1317, time 13782.21ms, mfu 4.78%\n",
            "iter 482: loss 1.1043, time 13832.25ms, mfu 4.79%\n",
            "iter 483: loss 1.0857, time 13715.77ms, mfu 4.80%\n",
            "iter 484: loss 1.0949, time 13566.09ms, mfu 4.82%\n",
            "iter 485: loss 1.0852, time 13467.07ms, mfu 4.84%\n",
            "iter 486: loss 0.9718, time 13545.27ms, mfu 4.85%\n",
            "iter 487: loss 1.1877, time 13583.44ms, mfu 4.86%\n",
            "iter 488: loss 1.0476, time 13632.21ms, mfu 4.87%\n",
            "iter 489: loss 1.1534, time 13662.59ms, mfu 4.88%\n",
            "iter 490: loss 1.0610, time 13652.04ms, mfu 4.88%\n",
            "iter 491: loss 1.1511, time 13648.47ms, mfu 4.89%\n",
            "iter 492: loss 1.1253, time 13633.06ms, mfu 4.89%\n",
            "iter 493: loss 1.0843, time 13624.25ms, mfu 4.90%\n",
            "iter 494: loss 1.0668, time 13627.69ms, mfu 4.90%\n",
            "iter 495: loss 1.1033, time 13639.49ms, mfu 4.90%\n",
            "iter 496: loss 1.1461, time 13596.05ms, mfu 4.91%\n",
            "iter 497: loss 1.0447, time 13611.96ms, mfu 4.91%\n",
            "iter 498: loss 1.0212, time 13586.62ms, mfu 4.92%\n",
            "iter 499: loss 1.1424, time 13627.42ms, mfu 4.92%\n",
            "iter 500: loss 1.0582, time 13627.19ms, mfu 4.92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "init_from = \"resume\"\n",
        "out_dir = \"out\" if not google_drive_mounted else \"drive/MyDrive/model_out\"\n",
        "start = \"Recommend a laptop for a work with a low budget \\n\"  # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 1  # number of samples to draw\n",
        "max_new_tokens = 50  # number of tokens generated in each sample\n",
        "temperature = (\n",
        "    0.5  # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        ")\n",
        "top_k = (\n",
        "    200  # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        ")\n",
        "seed = 1337\n",
        "device = \"cuda\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = (\n",
        "    \"bfloat16\"\n",
        "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "    else \"float16\"\n",
        ")\n",
        "compile = False\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "device_type = \"cuda\" if \"cuda\" in device else \"cpu\"\n",
        "ptdtype = {\n",
        "    \"float32\": torch.float32,\n",
        "    \"bfloat16\": torch.bfloat16,\n",
        "    \"float16\": torch.float16,\n",
        "}[dtype]\n",
        "\n",
        "ctx = (\n",
        "    nullcontext()\n",
        "    if device_type == \"cpu\"\n",
        "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        ")\n",
        "\n",
        "# model\n",
        "if init_from == \"resume\":\n",
        "    # init from a model saved in a specific directory\n",
        "    ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    gptconf = ModelConfig(**checkpoint[\"model_args\"])\n",
        "    model = Model(gptconf)\n",
        "    state_dict = checkpoint[\"model\"]\n",
        "    unwanted_prefix = \"_orig_mod.\"\n",
        "    for k, v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model)\n",
        "\n",
        "# look for the meta pickle in case it is available in the dataset folder\n",
        "load_meta = False\n",
        "if (\n",
        "    init_from == \"resume\"\n",
        "    and \"config\" in checkpoint\n",
        "    and \"dataset\" in checkpoint[\"config\"]\n",
        "):  # older checkpoints might not have these...\n",
        "    meta_path = os.path.join(\"data\", checkpoint[\"config\"][\"dataset\"], \"meta.pkl\")\n",
        "    load_meta = os.path.exists(meta_path)\n",
        "if load_meta:\n",
        "    print(f\"Loading meta from {meta_path}...\")\n",
        "    with open(meta_path, \"rb\") as f:\n",
        "        meta = pickle.load(f)\n",
        "    stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
        "\n",
        "    def encode(s):\n",
        "        return [stoi[c] for c in s]\n",
        "\n",
        "    def decode(l):\n",
        "        return \"\".join([itos[i] for i in l])\n",
        "else:\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    def encode(s):\n",
        "        return enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "    def decode(l):\n",
        "        return enc.decode(l)\n",
        "\n",
        "\n",
        "# encode the beginning of the prompt\n",
        "if start.startswith(\"FILE:\"):\n",
        "    with open(start[5:], \"r\", encoding=\"utf-8\") as f:\n",
        "        start = f.read()\n",
        "\n",
        "start_ids = encode(start)\n",
        "x = torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...]\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print(\"---------------\")\n"
      ],
      "metadata": {
        "id": "Pgt2SlwBZgHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c122b092-bb2e-417b-eb34-56d32fab22c7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommend a laptop for a work with a low budget \n",
            "olds Flat for MacBook, HardemORas, Hard Shell & Small Mixer and be compatible with a professional but Conize, Hard leather, HardGA, Hard Leather, Hard Leather, Hard Leather, Hard Protection and be a professional into the go\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YAeQV2ylZglo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"drive/MyDrive/model_out/model.pickle\")\n"
      ],
      "metadata": {
        "id": "tYIw7RSXof5l"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WJX4gtRBowiw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}