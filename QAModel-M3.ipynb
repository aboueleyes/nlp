{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FztqpkgMZhrc",
        "outputId": "6a6ea236-92be-494f-bc43-22fdba8295fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1L6a_lc4yzCYU_FTWYyy32anZoZtY3Q66\n",
            "To: /content/amazon_top500.csv\n",
            "100% 288k/288k [00:00<00:00, 82.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_IsZ5R1FWUJYZXFOh_wgCWDK7KLyWOBo\n",
            "To: /content/laptop.csv\n",
            "100% 137k/137k [00:00<00:00, 66.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1L6a_lc4yzCYU_FTWYyy32anZoZtY3Q66\n",
        "!gdown 1_IsZ5R1FWUJYZXFOh_wgCWDK7KLyWOBo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LRoAi69LMso",
        "outputId": "b063871b-a2c7-4402-8f44-9cb11f9e73bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAOVXO2kZlGi",
        "outputId": "97e35bc9-c83b-4cf6-9e79-50d3739d8c35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BiwWHyDLU-6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "toJI8Mw0Zn4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14fc72d9-a72c-4767-9375-6e487dc70722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'amazon_top500.csv': No such file or directory\n",
            "cp: cannot stat 'laptop.csv': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mkdir data/amazon -p\n",
        "!cp amazon_top500.csv data/amazon\n",
        "!cp laptop.csv data/amazon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dGLNsgl2aFHz"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 32000\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V9wEQmtDaJsP"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import logging\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False\"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n",
        "        if not self.flash:\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\n",
        "                \"bias\",\n",
        "                torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
        "                    1, 1, config.block_size, config.block_size\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = (\n",
        "            x.size()\n",
        "        )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
        "            1, 2\n",
        "        )  # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
        "            1, 2\n",
        "        )  # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
        "            1, 2\n",
        "        )  # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(\n",
        "                q,\n",
        "                k,\n",
        "                v,\n",
        "                attn_mask=None,\n",
        "                dropout_p=self.dropout if self.training else 0,\n",
        "                is_causal=True,\n",
        "            )\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = (\n",
        "            y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        )  # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            dict(\n",
        "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
        "                drop=nn.Dropout(config.dropout),\n",
        "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "                ln_f=LayerNorm(config.n_embd, bias=config.bias),\n",
        "            )\n",
        "        )\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith(\"c_proj.weight\"):\n",
        "                torch.nn.init.normal_(\n",
        "                    p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer)\n",
        "                )\n",
        "\n",
        "        # report number of parameters\n",
        "        logger.info(\"number of parameters: %.2fM\" % (self.get_num_params() / 1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)  # shape (t)\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1\n",
        "            )\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(\n",
        "                x[:, [-1], :]\n",
        "            )  # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        # decrease the block size if necessary\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(\n",
        "            self.transformer.wpe.weight[:block_size]\n",
        "        )\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, \"bias\"):\n",
        "                block.attn.bias = block.attn.bias[:, :, :block_size, :block_size]\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
        "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        logger.info(\n",
        "            f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\"\n",
        "        )\n",
        "        logger.info(\n",
        "            f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\"\n",
        "        )\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            optim_groups, lr=learning_rate, betas=betas, **extra_args\n",
        "        )\n",
        "        logger.info(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\"estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS\"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd // cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6 * N + 12 * L * H * Q * T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0 / dt)  # per second\n",
        "        flops_promised = 312e12  # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = (\n",
        "                idx\n",
        "                if idx.size(1) <= self.config.block_size\n",
        "                else idx[:, -self.config.block_size :]\n",
        "            )\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {} # default to empty dict\n",
        "        # only dropout can be overridden see more notes below\n",
        "        assert all(k == 'dropout' for k in override_args)\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        config_args['bias'] = True # always True for GPT model checkpoints\n",
        "        # we can override the dropout rate, if desired\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = ModelConfig(**config_args)\n",
        "        model = Model(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "2IIa309FZWRT"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "DATASET_PATH = \"/content/data/amazon/amazon_top500.csv\"\n",
        "\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "df2= pd.read_csv(\"/content/data/amazon/laptop.csv\")\n",
        "\n",
        "# Create a formatted string with header and rows\n",
        "df_str = \" \".join(df.columns) + \"\\n\"  # Add headers\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    row_str = \", \".join([f\"{header} : {value}\" for header, value in row.items()])\n",
        "    df_str += row_str + \"\\n\"\n",
        "\n",
        "for index, row in df2.iterrows():\n",
        "    row_str = \", \".join([f\"{header} : {value}\" for header, value in row.items()])\n",
        "    df_str += row_str + \"\\n\"\n",
        "\n",
        "\n",
        "\n",
        "# Write the formatted string to a text file\n",
        "output_path = \"/content/data/amazon/laptobs.txt\"\n",
        "\n",
        "with open(output_path, \"w\") as f:\n",
        "    f.write(df_str)\n",
        "\n",
        "\n",
        "with open(DATASET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "n = len(data)\n",
        "logger.info(f\"Data has {n:,} characters\")\n",
        "train_data = data[: int(n * 0.9)]\n",
        "val_data = data[int(n * 0.9) :]\n",
        "\n",
        "# encode with tiktoken gpt2 bpe\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "train_ids = enc.encode_ordinary(train_data)\n",
        "val_ids = enc.encode_ordinary(val_data)\n",
        "\n",
        "logger.info(f\"train has {len(train_ids):,} tokens\")\n",
        "logger.info(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "logger.info(\"Exporting to bin files\")\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "\n",
        "train_ids.tofile(\"/content/data/amazon/train.bin\")\n",
        "val_ids.tofile(\"/content/data/amazon/val.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xtFSrmvSaVsV",
        "outputId": "3e63eb91-e392-475b-b69c-8ca48814b5b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 245,760\n",
            "Resuming training from drive/MyDrive/model_out\n",
            "compiling the model... (takes a ~minute)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Process ForkProcess-1:\n",
            "Process ForkProcess-2:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
            "    res = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ad08f7aae8db>\u001b[0m in \u001b[0;36m<cell line: 280>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;31m# evaluate the loss on train/val sets and write checkpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaster_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         print(\n\u001b[1;32m    290\u001b[0m             \u001b[0;34mf\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-ad08f7aae8db>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                 \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mdynamo_config_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-92768a4ff151>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mdynamo_config_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0mfull_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mfull_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;31m# Just for convenience\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_boxed_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boxed_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36mruntime_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 all_outs = call_func_at_runtime_with_args(\n\u001b[0m\u001b[1;32m     95\u001b[0m                     \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py\u001b[0m in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_boxed_call\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m# TODO: Please remove soon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\u001b[0m in \u001b[0;36mrng_functionalization_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaybe_subclass_meta\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_current_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\u001b[0m in \u001b[0;36m_run_from_cache\u001b[0;34m(compiled_graph, inputs)\u001b[0m\n\u001b[1;32m    890\u001b[0m         ).call\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/torchinductor_root/3f/c3fagcli7n6t7mmwaqzhcr3ehazyoikgyih6bvda4ruh6yrmyi3w.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   2506\u001b[0m     \u001b[0mbuf47\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf26\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mbuf26\u001b[0m  \u001b[0;31m# reuse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m     \u001b[0;31m# Source Nodes: [l__self___transformer_h_2_attn_c_attn], Original ATen: [aten.addmm]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2508\u001b[0;31m     \u001b[0mextern_kernels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg69_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreinterpret_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf46\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m6144\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreinterpret_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg68_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2304\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuf47\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2509\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0marg68_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0marg69_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.distributed import destroy_process_group, init_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "\n",
        "# I/O\n",
        "google_drive_mounted = True\n",
        "out_dir = \"out\" if not google_drive_mounted else \"drive/MyDrive/model_out\"\n",
        "eval_interval = 32\n",
        "log_interval = 1\n",
        "eval_iters = 3\n",
        "eval_only = False  # if True, script exits right after the first eval\n",
        "always_save_checkpoint = True  # if True, always save a checkpoint after each eval\n",
        "init_from = \"resume\"  # 'scratch' or 'resume' or  'gpt2*'\n",
        "\n",
        "# wandb logging\n",
        "wandb_log = False  # disabled by default\n",
        "wandb_project = \"owt\"\n",
        "wandb_run_name = \"gpt2\"  # 'run' + str(time.time())\n",
        "\n",
        "# data\n",
        "dataset = \"amazon\"\n",
        "gradient_accumulation_steps = 5 * 8  # used to simulate larger batch sizes\n",
        "batch_size = 6  # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 1024\n",
        "\n",
        "# model\n",
        "n_layer = 12\n",
        "n_head = 12\n",
        "n_embd = 768\n",
        "dropout = 0.0  # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
        "\n",
        "# adamw optimizer\n",
        "learning_rate = 6e-4  # max learning rate\n",
        "max_iters = 500  # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0  # clip gradients at this value, or disable if == 0.0\n",
        "\n",
        "# learning rate decay settings\n",
        "decay_lr = True  # whether to decay the learning rate\n",
        "warmup_iters = 5  # how many steps to warm up for\n",
        "lr_decay_iters = 500  # should be ~= max_iters per Chinchilla\n",
        "min_lr = 6e-5  # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "\n",
        "# DDP settings\n",
        "backend = \"nccl\"  # 'nccl', 'gloo', etc.\n",
        "\n",
        "# system\n",
        "device = \"cpu\"\n",
        "\n",
        "dtype = (\n",
        "    \"bfloat16\"\n",
        "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "    else \"float16\"\n",
        ")  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True  # use PyTorch 2.0 to compile the model to be faster\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [\n",
        "    k\n",
        "    for k, v in globals().items()\n",
        "    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n",
        "]\n",
        "config = {k: globals()[k] for k in config_keys}\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "ddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?\n",
        "\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ[\"RANK\"])\n",
        "    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
        "    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "    device = f\"cuda:{ddp_local_rank}\"\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0  # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank  # each process gets a different seed\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
        "device_type = \"cuda\" if \"cuda\" in device else \"cpu\"\n",
        "\n",
        "ptdtype = {\n",
        "    \"float32\": torch.float32,\n",
        "    \"bfloat16\": torch.bfloat16,\n",
        "    \"float16\": torch.float16,\n",
        "}[dtype]\n",
        "\n",
        "ctx = (\n",
        "    nullcontext()\n",
        "    if device_type == \"cpu\"\n",
        "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        ")\n",
        "\n",
        "\n",
        "data_dir = os.path.join(\"data\", dataset)\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    if split == \"train\":\n",
        "        data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack(\n",
        "        [torch.from_numpy((data[i : i + block_size]).astype(np.int64)) for i in ix]\n",
        "    )\n",
        "    y = torch.stack(\n",
        "        [\n",
        "            torch.from_numpy((data[i + 1 : i + 1 + block_size]).astype(np.int64))\n",
        "            for i in ix\n",
        "        ]\n",
        "    )\n",
        "    if device_type == \"cuda\":\n",
        "        x, y = (\n",
        "            x.pin_memory().to(device, non_blocking=True),\n",
        "            y.pin_memory().to(device, non_blocking=True),\n",
        "        )\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(data_dir, \"meta.pkl\")\n",
        "meta_vocab_size = None\n",
        "\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, \"rb\") as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta[\"vocab_size\"]\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "model_args = dict(\n",
        "    n_layer=n_layer,\n",
        "    n_head=n_head,\n",
        "    n_embd=n_embd,\n",
        "    block_size=block_size,\n",
        "    bias=bias,\n",
        "    vocab_size=None,\n",
        "    dropout=dropout,\n",
        ")\n",
        "\n",
        "if init_from == \"scratch\":\n",
        "    # init a new model from scratch\n",
        "    model_args[\"vocab_size\"] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "\n",
        "    model_conf = ModelConfig(**model_args)\n",
        "    model = Model(model_conf)\n",
        "\n",
        "elif init_from == \"resume\":\n",
        "    print(f\"Resuming training from {out_dir}\")\n",
        "\n",
        "    ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint[\"model_args\"]\n",
        "    # force these config attributes to be equal otherwise we can't even resume training\n",
        "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
        "    for k in [\"n_layer\", \"n_head\", \"n_embd\", \"block_size\", \"bias\", \"vocab_size\"]:\n",
        "        model_args[k] = checkpoint_model_args[k]\n",
        "\n",
        "    model_conf = ModelConfig(**model_args)\n",
        "    model = Model(model_conf)\n",
        "    state_dict = checkpoint[\"model\"]\n",
        "    unwanted_prefix = \"_orig_mod.\"\n",
        "    for k, v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "    iter_num = checkpoint[\"iter_num\"]\n",
        "    best_val_loss = checkpoint[\"best_val_loss\"]\n",
        "\n",
        "elif init_from.startswith('gpt2'):\n",
        "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
        "    # initialize from  GPT-2 weights\n",
        "    override_args = dict(dropout=dropout)\n",
        "    model = Model.from_pretrained(init_from, override_args)\n",
        "    # read off the created config params, so we can store them into checkpoint correctly\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = getattr(model.config, k)\n",
        "\n",
        "# crop down the model block size if desired, using model surgery\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args[\"block_size\"] = (\n",
        "        block_size\n",
        "    )\n",
        "model.to(device)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay, learning_rate, (beta1, beta2), device_type\n",
        ")\n",
        "if init_from == \"resume\":\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "checkpoint = None  # free up memory\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    un_optimized_model = model\n",
        "    model = torch.compile(model)\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "\n",
        "\n",
        "# training loop\n",
        "X, Y = get_batch(\"train\")  # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0\n",
        "raw_model = model.module if ddp else model\n",
        "running_mfu = -1.0\n",
        "\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(\n",
        "            f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
        "        )\n",
        "        if wandb_log:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"iter\": iter_num,\n",
        "                    \"train/loss\": losses[\"train\"],\n",
        "                    \"val/loss\": losses[\"val\"],\n",
        "                    \"lr\": lr,\n",
        "                    \"mfu\": running_mfu * 100,  # convert to percentage\n",
        "                }\n",
        "            )\n",
        "        if losses[\"val\"] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses[\"val\"]\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    \"model\": raw_model.state_dict(),\n",
        "                    \"optimizer\": optimizer.state_dict(),\n",
        "                    \"model_args\": model_args,\n",
        "                    \"iter_num\": iter_num,\n",
        "                    \"best_val_loss\": best_val_loss,\n",
        "                    \"config\": config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, \"ckpt.pt\"))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            model.require_backward_grad_sync = (\n",
        "                micro_step == gradient_accumulation_steps - 1\n",
        "            )\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = (\n",
        "                loss / gradient_accumulation_steps\n",
        "            )  # scale the loss to account for gradient accumulation\n",
        "        X, Y = get_batch(\"train\")\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5:  # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
        "        print(\n",
        "            f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\"\n",
        "        )\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def extract_laptop_specs(text):\n",
        "    # Define regex patterns for various laptop specifications with capturing groups\n",
        "    specs_patterns = {\n",
        "        \"Model\": re.compile(r'(Dell|Mackbook|Lenovo|HP|Asus)', re.IGNORECASE),\n",
        "        \"Dimensions\": re.compile(r'Dimensions?\\s*:\\s*([\\d\\w\\.\\(\\) x]+)', re.IGNORECASE),\n",
        "        \"Compatibility\": re.compile(r'Compatible with\\s*:\\s*([\\w\\s\\\"\\.\\/\\(\\)]+)', re.IGNORECASE),\n",
        "        \"Screen Size\": re.compile(r'(\\d{1,2}\\.\\d{1,2})\\s*(?:inch|\")', re.IGNORECASE),\n",
        "        \"Release Year\": re.compile(r'\\b(20\\d{2})\\b', re.IGNORECASE),\n",
        "        \"Chip\": re.compile(r'\\b(M1|M2|Intel|AMD)\\b', re.IGNORECASE),\n",
        "        \"Price\": re.compile(r'\\$[\\s,-]*([\\d\\.,]+)', re.IGNORECASE),\n",
        "        \"Weight\": re.compile(r'Weight\\s*:\\s*([\\d\\.]+\\s*(?:kg|lb))', re.IGNORECASE),\n",
        "        \"Color\": re.compile(r'Color\\s*:\\s*([\\w\\s]+)', re.IGNORECASE),\n",
        "        \"Refresh Rate\": re.compile(r'(\\d{2,3})\\s*Hz', re.IGNORECASE),\n",
        "        \"Display\": re.compile(r'(\\d{3,4}p)', re.IGNORECASE),\n",
        "        \"Charger Wattage\": re.compile(r'(\\d{2,3})W\\s*Charging', re.IGNORECASE),\n",
        "        \"Water Proof\": re.compile(r'(Water)', re.IGNORECASE),\n",
        "        \"Girly\" : re.compile(r'(Girl|Women|cute)',  re.IGNORECASE),\n",
        "        \"Number of pieces\": re.compile(r'(\\d+)\\s*pcs', re.IGNORECASE)\n",
        "    }\n",
        "\n",
        "    # Extract details using the regex patterns\n",
        "    specs = {}\n",
        "    for key, pattern in specs_patterns.items():\n",
        "        match = pattern.search(text)\n",
        "        if match:\n",
        "            value = match.group(0).strip()  # Strip any leading/trailing spaces\n",
        "            specs[key] = value\n",
        "\n",
        "    return specs\n",
        "\n",
        "\n",
        "def post_process_output(output):\n",
        "      specs = extract_laptop_specs(output)\n",
        "      processed_text = \"Model: {}\\n\".format(specs['Model'].strip()) if 'Model' in specs else \"\"\n",
        "      if 'Dimensions' in specs:\n",
        "          processed_text += \"Dimensions: {}\\n\".format(specs['Dimensions'])\n",
        "      if 'Compatibility' in specs:\n",
        "          processed_text += \"Compatibility: {}\\n\".format(specs['Compatibility'])\n",
        "      if 'Screen Size' in specs:\n",
        "          processed_text += \"Screen Size: {}\\n\".format(specs['Screen Size'].strip())\n",
        "      if 'Release Year' in specs:\n",
        "          processed_text += \"Release Year: {}\\n\".format(specs['Release Year'])\n",
        "      if 'Chip' in specs:\n",
        "          processed_text += \"Chip: {}\\n\".format(specs['Chip'])\n",
        "      if 'Price' in specs:\n",
        "          processed_text += \"Price: ${}\\n\".format(specs['Price'].split(',')[1])\n",
        "      if 'Weight' in specs:\n",
        "          processed_text += \"Weight: {}\\n\".format(specs['Weight'])\n",
        "      if 'Color' in specs:\n",
        "          processed_text += \"Color: {}\\n\".format(specs['Color'])\n",
        "      if 'Refresh Rate' in specs:\n",
        "          processed_text += \"Refresh Rate: {}\\n\".format(specs['Refresh Rate'])\n",
        "      if 'Display' in specs:\n",
        "          processed_text += \"Display: {}\\n\".format(specs['Display'])\n",
        "      if 'Charger Wattage' in specs:\n",
        "          processed_text += \"Charger Wattage: {}\\n\".format(specs['Charger Wattage'])\n",
        "      if 'Water Proof' in specs:\n",
        "          processed_text += \"Water Proof: Yes\\n\"\n",
        "      if 'Girly' in specs:\n",
        "          processed_text += \"Girly: Yes\\n\"\n",
        "      if 'Number of pieces' in specs:\n",
        "          processed_text += f'Number of pieces: {specs[\"Number of pieces\"]}\\n'\n",
        "\n",
        "\n",
        "\n",
        "      processed_text += \"---------------\"\n",
        "      return processed_text"
      ],
      "metadata": {
        "id": "uBjbzLr1vjsr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pgt2SlwBZgHQ",
        "outputId": "e58057d8-74ac-441b-cba7-e68eacdc591d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suggest a Charger for Dell \n",
            "Dell 65W 19.5V 3.34A Ac Adapter Charger Power Supply for Dell Latitude E6420 E6430 E6430s E6430U E6440 E6500 E6510 E6520 E6530 E6540 E7240 E7250\n",
            "Model: Dell\n",
            "---------------\n",
            "---------------\n",
            "Suggest a Charger for Dell \n",
            "\"(3Pcs) Gifts for Autism Autistic Sticker Awareness Autism Pride Decorate Books Laptops Water Bottles Kindle Bookish Sticker Waterproof Vinyl Stickers Vintage Retro Groovy Design Size 3\"\"x1.26\"\" Inch\",TADAVAX,,$,3.69,,\n",
            "Model: Dell\n",
            "Screen Size: 1.26\"\n",
            "Price: $3.69\n",
            "Water Proof: Yes\n",
            "Number of pieces: 3Pcs\n",
            "---------------\n",
            "---------------\n",
            "Suggest a Charger for Dell \n",
            "\"50PCS Singer Stickers, Vinyl Waterproof Stickers for Girls, Music Albums Stickers Decoration, Rock Band Stickers, Skateboard, Laptop, Water Bottles, Scrapbook for Fans\",wonfurd,,$,5.99,4.8,44\n",
            "Model: Dell\n",
            "Price: $5.99\n",
            "Water Proof: Yes\n",
            "Girly: Yes\n",
            "Number of pieces: 50PCS\n",
            "---------------\n",
            "---------------\n",
            "Suggest a Charger for Dell \n",
            "USB Docking Station Dual Monitor (Dual Video HDMI & VGA, Gigabit Ethernet, Audio, and More USB Ports)-Grey\",egiant,Laptop Backpack,$,29.99,4.4,106.0\n",
            "\"SlimQ 150W GaN Gaming Laptop Charg\n",
            "Model: Dell\n",
            "Screen Size: 06.0\n",
            "\"\n",
            "Price: $29.99\n",
            "---------------\n",
            "---------------\n",
            "Suggest a Charger for Dell \n",
            "\"STOON Laptop Stand Adjustable Height, Ergonomic Portable Laptop Holder Riser for Desk, Aluminum Foldable Computer Notebook Stand Compatible with MacBook Air Pro, Dell XPS, HP (10-16\"\") -Black\",STOON,,$,22.99,\n",
            "Model: Dell\n",
            "Price: $22.99\n",
            "---------------\n",
            "---------------\n",
            "Suggest a Charger for Dell \n",
            "Dell 65W 19.5V 3.34A Ac Adapter Charger Power Supply for Dell Latitude E6420 E6430 E6430s E6430U E6440 E6500 E6510 E6520 E6530 E6540 E7240 E7250\n",
            "Model: Dell\n",
            "---------------\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "init_from = \"resume\"\n",
        "google_drive_mounted = True\n",
        "out_dir = \"out\" if not google_drive_mounted else \"drive/MyDrive/model_out\"\n",
        "start = \"Suggest a Charger for Dell \\n\"  # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 6  # number of samples to draw\n",
        "max_new_tokens = 60  # number of tokens generated in each sample\n",
        "temperature = (\n",
        "    1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        ")\n",
        "top_k = (\n",
        "    200  # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        ")\n",
        "seed = 1337\n",
        "device = \"cuda\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = (\n",
        "    \"bfloat16\"\n",
        "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "    else \"float16\"\n",
        ")\n",
        "compile = False\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "device_type = \"cuda\" if \"cuda\" in device else \"cpu\"\n",
        "ptdtype = {\n",
        "    \"float32\": torch.float32,\n",
        "    \"bfloat16\": torch.bfloat16,\n",
        "    \"float16\": torch.float16,\n",
        "}[dtype]\n",
        "\n",
        "ctx = (\n",
        "    nullcontext()\n",
        "    if device_type == \"cpu\"\n",
        "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        ")\n",
        "\n",
        "# model\n",
        "if init_from == \"resume\":\n",
        "    # init from a model saved in a specific directory\n",
        "    ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    gptconf = ModelConfig(**checkpoint[\"model_args\"])\n",
        "    model = Model(gptconf)\n",
        "    state_dict = checkpoint[\"model\"]\n",
        "    unwanted_prefix = \"_orig_mod.\"\n",
        "    for k, v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model)\n",
        "\n",
        "# look for the meta pickle in case it is available in the dataset folder\n",
        "load_meta = False\n",
        "if (\n",
        "    init_from == \"resume\"\n",
        "    and \"config\" in checkpoint\n",
        "    and \"dataset\" in checkpoint[\"config\"]\n",
        "):  # older checkpoints might not have these...\n",
        "    meta_path = os.path.join(\"data\", checkpoint[\"config\"][\"dataset\"], \"meta.pkl\")\n",
        "    load_meta = os.path.exists(meta_path)\n",
        "if load_meta:\n",
        "    print(f\"Loading meta from {meta_path}...\")\n",
        "    with open(meta_path, \"rb\") as f:\n",
        "        meta = pickle.load(f)\n",
        "    stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
        "\n",
        "    def encode(s):\n",
        "        return [stoi[c] for c in s]\n",
        "\n",
        "    def decode(l):\n",
        "        return \"\".join([itos[i] for i in l])\n",
        "else:\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    def encode(s):\n",
        "        return enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "    def decode(l):\n",
        "        return enc.decode(l)\n",
        "\n",
        "\n",
        "# encode the beginning of the prompt\n",
        "if start.startswith(\"FILE:\"):\n",
        "    with open(start[5:], \"r\", encoding=\"utf-8\") as f:\n",
        "        start = f.read()\n",
        "\n",
        "start_ids = encode(start)\n",
        "x = torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...]\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            out = decode(y[0].tolist())\n",
        "            print(out)\n",
        "            print(post_process_output(out))\n",
        "            print(\"---------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAeQV2ylZglo"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYIw7RSXof5l"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"drive/MyDrive/model_out/model.pickle\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJX4gtRBowiw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}